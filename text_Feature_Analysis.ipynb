{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis of Website Textual and Imagery Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pymongo\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.core.display import display, HTML\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = MongoClient(open('../db.txt', encoding='UTF-8').read())\n",
    "db = client.knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = db.labels\n",
    "ct = labels.count_documents({'text':{'$ne':None}})\n",
    "print(ct)\n",
    "\n",
    "firstText = labels.find_one({'text':{'$ne':None}})\n",
    "type(firstText['text'])\n",
    "# Image.frombytes(mode='RGB',size=(1000,760),data=firstImage['image'])\n",
    "\n",
    "# images = [label['image'] for label in labels.find({'image':{'$ne':None}})]\n",
    "# print(len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = stopwords.words(\"english\")\n",
    "extra_words = []\n",
    "words_to_remove += extra_words\n",
    "print(words_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_words = html.map(lambda r: [re.sub(\"[^a-zA-Z]\", \"\", word) for word in r.lower().split() if not word in words_to_remove])\n",
    "interesting_words = interesting_words.map(lambda words: [w for w in words if not w in words_to_remove])\n",
    "\n",
    "intersting_words = interesting_words.map( lambda words: [w for w in words if re.sub('[aeiouy]','',w) != w])\n",
    "html = interesting_words.map(lambda words: [w for w in words if len(w)])\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vocab = Countvectorizer.get_feature_names()\n",
    "print('There are %d number of words in our vocab' % len(count_vocab))\n",
    "\n",
    "dist = np.sum(count_train_data_features, axis=0)\n",
    "\n",
    "hist = [];\n",
    "for tag, count in zip(count_vocab, dist):\n",
    "    hist.append((count, tag))\n",
    "    \n",
    "print(\"The top ten word list is is\")\n",
    "\n",
    "# IMPORT pprint IN THE IMPORT BLOCK\n",
    "pprint.pprint(sorted(hist, key= lambda x : x[0], reverse=True)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-a3141848b0f8>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-a3141848b0f8>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    wordCount++\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "html = []\n",
    "wordList = {}\n",
    "html = html.replace('\\t',' ')\n",
    "html = html.replace('\\n',' ')\n",
    "soup = BeautifulSoup(result, 'html.parser')\n",
    "tag = [title, div, p, span, a]\n",
    "anotherList = []\n",
    "for i in tag:\n",
    "    html.push(soup.tag[i].string.split(' '))\n",
    "#count the number of times that a word occurs by scraping the list\n",
    "count = 0\n",
    "wordCount = 0\n",
    "for i in html:\n",
    "    for x in range(1,len(html)+1):\n",
    "        if html[count] == html[x]:\n",
    "            wordCount++\n",
    "            if html.length = x-1:\n",
    "                wordList.update({i:wordCount})\n",
    "    count++\n",
    "\n",
    "for z in range(len(wordList)):\n",
    "    word = wordList[wordList[z]]\n",
    "    anotherList.push(word)\n",
    "anotherList.sort()\n",
    "\n",
    "count1 = 0\n",
    "t = True\n",
    "for i in anotherList:\n",
    "    while t == True:\n",
    "        if anotherList[i] == wordList[wordList[count1]]:\n",
    "            #move the set to the front of the dictionary: l.insert(newindex, l.pop(wordlist[count1]))\n",
    "            t = False\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
